{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[15, 2, 100],\n",
    "              [4, 5, 6],\n",
    "              [7, 35, 9]])\n",
    "np.argmax(a, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for fc1.W\n",
      "Gradient check passed!\n",
      "Checking gradient for fc1.B\n",
      "Gradient check passed!\n",
      "Checking gradient for fc2.W\n",
      "Gradient check passed!\n",
      "Checking gradient for fc2.B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for fc1.W\n",
      "Gradient check passed!\n",
      "Checking gradient for fc1.B\n",
      "Gradient check passed!\n",
      "Checking gradient for fc2.W\n",
      "Gradient check passed!\n",
      "Checking gradient for fc2.B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09999999999999998"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302127, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302394, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301986, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301993, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302349, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302823, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302845, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302590, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302251, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.302206, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301305, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302186, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302543, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302015, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302032, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302224, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302817, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302616, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302736, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302588, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down and train and val accuracy go up for every epoch\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c1593005f8>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.318994, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309078, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.316366, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304960, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285463, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278575, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.311836, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274429, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300407, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301126, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300140, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291873, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289480, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.198132, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.294460, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279775, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298176, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261416, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274693, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297895, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
